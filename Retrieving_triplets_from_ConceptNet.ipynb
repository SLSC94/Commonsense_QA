{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook produces a dictionary where the vocabulary found in the Questions in CommonsenseQA are keys and their triplets which are found in ConceptNet are the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "import re\n",
    "import requests\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep\n",
    "import time\n",
    "import multiprocessing\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(which_dataset): #train / dev / test    \n",
    "    questions = []\n",
    "    with open('data/' + which_dataset + '_qtoken_split.jsonl', 'rb') as f: # opening file in binary(rb) mode    \n",
    "        for item in json_lines.reader(f):\n",
    "            questions.append(item)\n",
    "    return questions\n",
    "\n",
    "def build_vocab(questions): #what we get from load_files\n",
    "    regex = r'\\b\\w+\\b'\n",
    "    vocab = []\n",
    "    for Q in questions:\n",
    "        vocab += re.findall(regex, Q['question']['stem'].lower())\n",
    "        \n",
    "        for i in range(3):\n",
    "            vocab += re.findall(regex, Q['question']['choices'][i]['text'].lower() )\n",
    "            \n",
    "    vocab = list(set(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use the API for ConceptNet to obtain download the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all of one word's triplet\n",
    "def get_triplets_single(word):\n",
    "\n",
    "    #This is the API. \n",
    "    obj = requests.get('http://api.conceptnet.io/c/en/' + word).json()\n",
    "\n",
    "    return(word, obj)\n",
    "\n",
    "\n",
    "def get_nodes(dictionary): #keys are vocabulary, values are the dicitonaries pulled from API\n",
    "    nodes_dict = {}\n",
    "    for word in dictionary.keys():\n",
    "        obj = dictionary[word]\n",
    "        \n",
    "        if 'error' not in obj.keys(): #eliminate words not found in ConceptNet\n",
    "            list_start = []\n",
    "            list_rels = []\n",
    "            list_end = []\n",
    "            list_lang_start = []\n",
    "            list_lang_end = []\n",
    "\n",
    "            len_edges = len(obj['edges'])\n",
    "            for j in range(len_edges):\n",
    "                '''Starting nodes'''\n",
    "                list_start.append(obj['edges'][j]['start']['label'])\n",
    "                #sometimes there might not be the 'language' key, so we must catch the exception. \n",
    "                try:\n",
    "                    list_lang_start.append(obj['edges'][j]['start']['language'])\n",
    "                except:\n",
    "                    list_lang_start.append('not en')\n",
    "                \n",
    "                '''End nodes'''\n",
    "                list_end.append(obj['edges'][j]['end']['label'])\n",
    "                #sometimes there might not be the 'language' key, so we must catch the exception. \n",
    "                try:\n",
    "                    list_lang_end.append(obj['edges'][j]['end']['language'])\n",
    "                except:\n",
    "                    list_lang_end.append('not en')\n",
    "\n",
    "                list_rels.append(obj['edges'][j]['rel']['label'])\n",
    "            #we will extract triplets which are exclusively in english because 'murica\n",
    "            relations = [(start,rel,end) for start,rel,end,lang_start,lang_end in zip(list_start,\n",
    "                                                                                       list_rels,\n",
    "                                                                                       list_end,\n",
    "                                                                                       list_lang_start,\n",
    "                                                                                       list_lang_end) if (lang_start == 'en' and lang_end == 'en')]\n",
    "            nodes_dict[word] = relations\n",
    "    return nodes_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using multithreading\n",
    "def get_triplets_multithread(vocab):\n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=16)\n",
    "    pool_outputs = pool.map(get_triplets_single,\n",
    "                            vocab)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    output = [x for x in pool_outputs if x is not None]\n",
    "    return (dict(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "def main(which_data):\n",
    "    data = load_files(which_data)\n",
    "    vocab = build_vocab(data)\n",
    "    triplets = get_triplets_multithread(vocab)\n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a decoder which will be used later\n",
    "def decoder(dictionary):\n",
    "    for key in dictionary.keys():\n",
    "        items = dictionary[key]\n",
    "        dictionary[key] = [tuple(item) for item in items]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run main() for 'dev' and 'test' below, then wait for one hour. \n",
    "\n",
    "### Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dictionary = main('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the triplets from the dictionaries\n",
    "dev_triplets = get_nodes(dev_dictionary)\n",
    "\n",
    "\n",
    "'''store both the raw dictionary and the triplets'''\n",
    "json.dump(dev_dictionary, open(\"data/dev_dictionary.txt\",'w'))\n",
    "json.dump(dev_triplets, open(\"data/dev_triplets.txt\",'w'))\n",
    "\n",
    "'''How to load the raw dictionary and the triplets'''\n",
    "#don't need the decoder for dev_dictionary\n",
    "temp_dev_dictionary = json.load(open(\"data/dev_dictionary.txt\"))\n",
    "#require the decoder for dev_triplets\n",
    "temp_dev_triplets = decoder(json.load(open(\"data/dev_triplets.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_dev_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dictionary = main('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the triplets from the dictionaries\n",
    "test_triplets = get_nodes(test_dictionary)\n",
    "\n",
    "\n",
    "'''store both the raw dictionary and the triplets'''\n",
    "json.dump(test_dictionary, open(\"test_dictionary.txt\",'w'))\n",
    "json.dump(test_triplets, open(\"test_triplets.txt\",'w'))\n",
    "\n",
    "'''How to load the raw dictionary and the triplets'''\n",
    "#don't need the decoder for dev_dictionary\n",
    "temp_test_dictionary = json.load(open(\"data/test_dictionary.txt\"))\n",
    "#require the decoder for dev_triplets\n",
    "temp_test_triplets = decoder(json.load(open(\"data/test_triplets.txt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## break for an hour\n",
    "\n",
    "# We have to break up the training data since the API only allows 6000 requests per hour\n",
    "\n",
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9002\n"
     ]
    }
   ],
   "source": [
    "data = load_files('train')\n",
    "vocab = build_vocab(data)\n",
    "print(len(vocab))\n",
    "vocab1 = vocab[0:5000]\n",
    "vocab2 = vocab[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dictionary1 = get_triplets_multithread(vocab1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for an hour before running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dictionary2 = get_triplets_multithread(vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the two sets of training dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dictionary = {**train_dictionary1, **train_dictionary2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the triplets from the dictionaries\n",
    "train_triplets = get_nodes(train_dictionary)\n",
    "\n",
    "\n",
    "'''store both the raw dictionary and the triplets'''\n",
    "json.dump(train_dictionary, open(\"data/train_dictionary.txt\",'w'))\n",
    "json.dump(train_triplets, open(\"data/train_triplets.txt\",'w'))\n",
    "\n",
    "'''How to load the raw dictionary and the triplets'''\n",
    "#don't need the decoder for dev_dictionary\n",
    "temp_train_dictionary = json.load(open(\"data/train_dictionary.txt\"))\n",
    "#require the decoder for dev_triplets\n",
    "temp_train_triplets = decoder(json.load(open(\"data/train_triplets.txt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that they have remained unchanged after saving/loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_dev_dictionary == dev_dictionary)\n",
    "print(temp_test_dictionary == test_dictionary)\n",
    "print(temp_train_dictionary == train_dictionary)\n",
    "\n",
    "print(temp_dev_triplets == dev_triplets)\n",
    "print(temp_test_triplets == test_triplets)\n",
    "print(temp_train_triplets == train_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function queries the API for ConceptNet one by one (instead of multithreading). Just for Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_triplets(vocab):\n",
    "#     triplet_dictionary = {}\n",
    "#     for i, word in tqdm(enumerate(vocab)):\n",
    "        \n",
    "        \n",
    "#         #This is the API. \n",
    "#         obj = requests.get('http://api.conceptnet.io/c/en/' + word).json()\n",
    "        \n",
    "        \n",
    "#         if 'error' not in obj.keys(): #eliminate words not found in ConceptNet\n",
    "#             list_start = []\n",
    "#             list_rels = []\n",
    "#             list_end = []\n",
    "#             list_lang = []\n",
    "\n",
    "#             len_edges = len(obj['edges'])\n",
    "#             for j in range(len_edges):\n",
    "                \n",
    "#                 list_start.append(obj['edges'][j]['start']['label'])\n",
    "#                 list_lang.append(obj['edges'][j]['start']['language'])\n",
    "#                 list_end.append(obj['edges'][j]['end']['label'])\n",
    "#                 list_rels.append(obj['edges'][j]['rel']['label'])\n",
    "#             #we will extract triplets which are exclusively in english because 'murica\n",
    "#             relations = [(start,rel,end) for start,rel,end,lang in zip(list_start,\n",
    "#                                                                        list_rels,\n",
    "#                                                                        list_end,\n",
    "#                                                                        list_lang) if lang == 'en']\n",
    "#             triplet_dictionary[word] = relations\n",
    "#     return triplet_dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
