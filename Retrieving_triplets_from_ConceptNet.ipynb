{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook produces a dictionary where the vocabulary found in the Questions in CommonsenseQA are keys and their triplets which are found in ConceptNet are the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "import re\n",
    "import requests\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep\n",
    "import time\n",
    "import multiprocessing\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(which_dataset): #train / dev / test    \n",
    "    questions = []\n",
    "    with open(which_dataset + '_qtoken_split.jsonl', 'rb') as f: # opening file in binary(rb) mode    \n",
    "        for item in json_lines.reader(f):\n",
    "            questions.append(item)\n",
    "    return questions\n",
    "\n",
    "def build_vocab(questions): #what we get from load_files\n",
    "    regex = r'\\b\\w+\\b'\n",
    "    vocab = []\n",
    "    for Q in questions:\n",
    "        vocab += re.findall(regex, Q['question']['stem'].lower())\n",
    "        for i in range(3):\n",
    "            vocab += Q['question']['choices'][i]['text'].lower()\n",
    "    vocab = list(set(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use the API for ConceptNet to obtain triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all of one word's triplet\n",
    "def get_triplets_single(word):\n",
    "\n",
    "    #This is the API. \n",
    "    obj = requests.get('http://api.conceptnet.io/c/en/' + word).json()\n",
    "\n",
    "\n",
    "    if 'error' not in obj.keys(): #eliminate words not found in ConceptNet\n",
    "        list_start = []\n",
    "        list_rels = []\n",
    "        list_end = []\n",
    "        list_lang = []\n",
    "\n",
    "        len_edges = len(obj['edges'])\n",
    "        for j in range(len_edges):\n",
    "\n",
    "            list_start.append(obj['edges'][j]['start']['label'])\n",
    "            list_lang.append(obj['edges'][j]['start']['language'])\n",
    "            list_end.append(obj['edges'][j]['end']['label'])\n",
    "            list_rels.append(obj['edges'][j]['rel']['label'])\n",
    "        #we will extract triplets which are exclusively in english because 'murica\n",
    "        relations = [(start,rel,end) for start,rel,end,lang in zip(list_start,\n",
    "                                                                   list_rels,\n",
    "                                                                   list_end,\n",
    "                                                                   list_lang) if lang == 'en']\n",
    "        \n",
    "        return (word, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using multithreading\n",
    "def get_triplets_multithread(vocab):\n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=16)\n",
    "    pool_outputs = pool.map(get_triplets_single,\n",
    "                            vocab)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    output = [x for x in pool_outputs if x is not None]\n",
    "    return (dict(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "def main(which_data):\n",
    "    data = load_files(which_data)\n",
    "    vocab = build_vocab(data)[0:100]\n",
    "    triplets = get_triplets_multithread(vocab)\n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the two cells below, then wait for one hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the triplets\n",
    "dev_triplets = main('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triplets = main('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have to break up the training data since the API only allows 6000 requests per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_files('train')\n",
    "vocab = build_vocab(data)\n",
    "\n",
    "vocab1 = vocab[0:3600]\n",
    "vocab2 = vocab[3600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets1 = get_triplets_multithread(vocab1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for an hour before running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets2 = get_triplets_multithread(vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the two sets of training triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = {**train_triplets1, **train_triplets2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the triplets\n",
    "json.dump(dev_triplets, open(\"dev_triplets.txt\",'w'))\n",
    "json.dump(test_triplets, open(\"test_triplets.txt\",'w'))\n",
    "json.dump(train_triplets, open(\"train_triplets.txt\",'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the triplets\n",
    "temp_dev_triplets = json.load(open(\"dev_triplets.txt\"))\n",
    "temp_test_triplets = json.load(open(\"test_triplets.txt\"))\n",
    "temp_train_triplets = json.load(open(\"train_triplets.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert back to tuple instead of list (since JSON files cannot store tuples):\n",
    "def decoder(dictionary):\n",
    "    for key in dictionary.keys():\n",
    "        items = dictionary[key]\n",
    "        dictionary[key] = [tuple(item) for item in items]\n",
    "    return dictionary\n",
    "\n",
    "_dev_triplets = decoder(temp_dev_triplets)\n",
    "_test_triplets = decoder(temp_test_triplets)\n",
    "_train_triplets = decoder(temp_train_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that they have remained unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(_dev_triplets == dev_triplets)\n",
    "print(_test_triplets == test_triplets)\n",
    "print(_train_triplets == train_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function queries the API for ConceptNet one by one (instead of multithreading). Just for Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_triplets(vocab):\n",
    "#     triplet_dictionary = {}\n",
    "#     for i, word in tqdm(enumerate(vocab)):\n",
    "        \n",
    "        \n",
    "#         #This is the API. \n",
    "#         obj = requests.get('http://api.conceptnet.io/c/en/' + word).json()\n",
    "        \n",
    "        \n",
    "#         if 'error' not in obj.keys(): #eliminate words not found in ConceptNet\n",
    "#             list_start = []\n",
    "#             list_rels = []\n",
    "#             list_end = []\n",
    "#             list_lang = []\n",
    "\n",
    "#             len_edges = len(obj['edges'])\n",
    "#             for j in range(len_edges):\n",
    "                \n",
    "#                 list_start.append(obj['edges'][j]['start']['label'])\n",
    "#                 list_lang.append(obj['edges'][j]['start']['language'])\n",
    "#                 list_end.append(obj['edges'][j]['end']['label'])\n",
    "#                 list_rels.append(obj['edges'][j]['rel']['label'])\n",
    "#             #we will extract triplets which are exclusively in english because 'murica\n",
    "#             relations = [(start,rel,end) for start,rel,end,lang in zip(list_start,\n",
    "#                                                                        list_rels,\n",
    "#                                                                        list_end,\n",
    "#                                                                        list_lang) if lang == 'en']\n",
    "#             triplet_dictionary[word] = relations\n",
    "#     return triplet_dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
